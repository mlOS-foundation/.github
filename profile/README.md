# mlOS Foundation

**Building the Universal Operating System for Machine Learning**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Axon](https://img.shields.io/badge/Axon-v3.1.3-brightgreen)](https://github.com/mlOS-foundation/axon/releases)
[![Core](https://img.shields.io/badge/Core-v3.2.8--alpha-blue)](https://github.com/mlOS-foundation/core-releases)
[![E2E Tests](https://github.com/mlOS-foundation/system-test/actions/workflows/e2e-test.yml/badge.svg)](https://mlos-foundation.github.io/system-test/)

## ğŸ¯ Mission

We're creating the foundational infrastructure that makes ML models truly portable across any framework, hardware, and deployment environment. Just as POSIX made software portable across Unix systems, **MLOS makes ML models portable across any framework and deployment environment**.

## ğŸ”¬ Core Innovation

MLOS introduces a paradigm shift: **ML frameworks don't integrate with deployment platforms - they integrate with MLOS through a universal standard interface (SMI)**.

### The MLOS Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Layer                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Axon CLI   â”‚  â”‚  MLOS API    â”‚  â”‚   Plugins    â”‚          â”‚
â”‚  â”‚  (v3.1.3)    â”‚  â”‚(HTTP/gRPC/IPC)â”‚ â”‚ (SMI-based)  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                 â”‚
          â”‚ 1. Install &     â”‚ 2. Register     â”‚ 3. Execute
          â”‚    Convert       â”‚    with MLOS    â”‚    Inference
          â”‚                  â”‚                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MLOS Core Engine (v3.2.8-alpha)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Model Registry  â”‚  Plugin Registry  â”‚  Resource Mgr     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Standard Model Interface (SMI)                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚    ONNX Runtime Plugin (Multi-Type Tensor Support)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ Kernel-Level Optimizations
          â”‚ (US-63/865,176)
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Operating System Kernel                             â”‚
â”‚  â€¢ ML-aware scheduler                                            â”‚
â”‚  â€¢ Tensor memory management                                      â”‚
â”‚  â€¢ GPU resource orchestration                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Latest Features

### ğŸš€ Universal ONNX Conversion (Axon v3.x)

Axon now features **universal ONNX conversion** across all major ML repositories:

| Repository | Converter | Coverage | Example |
|------------|-----------|----------|---------|
| **Hugging Face** ğŸ¤— | optimum + torch.onnx | 100K+ models | `axon install hf/bert-base-uncased@latest` |
| **PyTorch Hub** ğŸ”¥ | TorchScript | Research | `axon install pytorch/vision/resnet50@latest` |
| **TensorFlow Hub** ğŸ§  | tf2onnx | Production | `axon install tfhub/google/universal-sentence-encoder/4@latest` |
| **ModelScope** ğŸ¨ | Auto-detect | Multimodal | `axon install modelscope/damo/cv_resnet50@latest` |

**Key Features:**
- âœ… **Smart Repository Routing**: Automatic converter selection based on model namespace
- âœ… **Multi-Strategy Fallbacks**: Multiple conversion methods for maximum compatibility
- âœ… **Docker-Based Conversion**: Zero Python dependencies on host machine
- âœ… **Vision Model Support**: Full ImageNet input support (224Ã—224Ã—3 RGB)

### âš¡ Enhanced ONNX Runtime Plugin (Core v3.x)

The built-in ONNX Runtime plugin now supports **universal inference**:

| Tensor Type | Support | Use Case |
|-------------|---------|----------|
| **int64** | âœ… | NLP token IDs (GPT-2, BERT, T5) |
| **float32** | âœ… | Vision models, embeddings |
| **int32** | âœ… | TensorFlow models |
| **bool** | âœ… | Attention masks |

**Advanced Features:**
- âœ… Named input parsing (`{"input_ids": [...], "attention_mask": [...]}`)
- âœ… Multi-input models (BERT, RoBERTa, T5)
- âœ… Dynamic shape handling
- âœ… Large input support (16MB+)

## ğŸ“Š E2E Validation

**ğŸ“ˆ [View Latest Test Report](https://mlos-foundation.github.io/system-test/)**

| Model | Category | Status | Inference Time |
|-------|----------|--------|----------------|
| GPT-2 | NLP | âœ… Pass | ~68ms |
| BERT | NLP | âœ… Pass | ~98ms |
| RoBERTa | NLP | âœ… Pass | ~85ms |
| ResNet-50 | Vision | âœ… Pass | ~45ms |
| ViT | Vision | âœ… Pass | ~120ms |
| MobileNetV2 | Vision | âœ… Pass | ~25ms |

## ğŸ—ï¸ Key Components

### ğŸ§  Axon - The Neural Pathway for ML Models

**Version: v3.1.3** | [Repository](https://github.com/mlOS-foundation/axon) | [Releases](https://github.com/mlOS-foundation/axon/releases)

- **Universal Model Installer**: Works with 80%+ of ML repositories
- **Universal ONNX Conversion**: Docker-based multi-framework converter
- **Model Package Format (MPF)**: Standardized `.axon` packages with `manifest.yaml`
- **Manifest-First Architecture**: Format-agnostic model execution

**Installation:**
```bash
curl -sSL axon.mlosfoundation.org | sh
```

### âš™ï¸ MLOS Core - Kernel-Level ML Runtime

**Version: v3.2.8-alpha** | [Releases](https://github.com/mlOS-foundation/core-releases)

- **Multi-Protocol APIs**: HTTP REST, gRPC, IPC (ultra-low latency)
- **Enhanced ONNX Plugin**: Multi-type tensor support, named inputs
- **Plugin Architecture**: Framework-agnostic via Standard Model Interface (SMI)
- **Resource Management**: Kernel-level optimization for ML workloads

### ğŸ§ª System Test - E2E Validation Framework

**Repository**: [system-test](https://github.com/mlOS-foundation/system-test)

- **Automated E2E Testing**: Full pipeline validation (Axon â†’ Core â†’ Inference)
- **Multi-Model Coverage**: NLP, Vision, and Multimodal models
- **GitHub Pages Reports**: [mlos-foundation.github.io/system-test](https://mlos-foundation.github.io/system-test/)
- **CI/CD Integration**: Scheduled and on-demand test runs

### ğŸ“ SMI Spec - Standard Model Interface

- **Universal Contract**: Framework-agnostic interface for ML models
- **Multi-Language Support**: C, Python, Go, JavaScript
- **Status**: ğŸ”’ Private - In Development

## ğŸš€ Quick Start

### Install & Run Inference

```bash
# 1. Install Axon CLI
curl -sSL axon.mlosfoundation.org | sh

# 2. Install a model (auto-converts to ONNX)
axon install hf/bert-base-uncased@latest

# 3. Register with MLOS Core
axon register hf/bert-base-uncased@latest

# 4. Run inference
curl -X POST http://localhost:8080/models/hf%2Fbert-base-uncased%40latest/inference \
  -H "Content-Type: application/json" \
  -d '{"input_ids": [101, 7592, 102], "attention_mask": [1, 1, 1]}'
```

### Vision Model Example

```bash
# Install vision model
axon install hf/google/vit-base-patch16-224@latest

# Run inference with ImageNet input (224Ã—224Ã—3)
curl -X POST http://localhost:8080/models/hf%2Fgoogle%2Fvit-base-patch16-224%40latest/inference \
  -H "Content-Type: application/json" \
  -d '{"pixel_values": [...]}'  # 150528 float values
```

## ğŸ“‹ Component Versions

| Component | Version | Status | Repository |
|-----------|---------|--------|------------|
| **Axon** | v3.1.3 | âœ… Stable | [axon](https://github.com/mlOS-foundation/axon) |
| **MLOS Core** | v3.2.8-alpha | âœ… Alpha | [core-releases](https://github.com/mlOS-foundation/core-releases) |
| **System Test** | Active | âœ… CI/CD | [system-test](https://github.com/mlOS-foundation/system-test) |
| **SMI Spec** | v1.0.0 | ğŸ”’ Private | [smi-spec](https://github.com/mlOS-foundation/smi-spec) |
| **MLOS Linux (Ubuntu)** | - | ğŸ”„ Planning | [mlos-linux-ubuntu](https://github.com/mlOS-foundation/mlos-linux-ubuntu) |
| **MLOS Linux (Flatcar)** | - | ğŸ”„ Planning | [mlos-linux-flatcar](https://github.com/mlOS-foundation/mlos-linux-flatcar) |
| **MLOS Kernel Patches** | - | ğŸ”„ Planning | [mlos-linux-kernel](https://github.com/mlOS-foundation/mlos-linux-kernel) |

## ğŸ“… Development Roadmap

### âœ… Phase 0: Foundation (Complete)
- Standard Model Interface (SMI) specification
- Axon v1.0.0 - Hugging Face Hub adapter
- MLOS Core v1.0.0 - Core runtime with HTTP API
- Patent applications (US-63/861,527, US-63/865,176)

### âœ… Phase 1: Universal Model Delivery (Complete)
- Multi-repository adapters (HuggingFace, PyTorch Hub, TF Hub, ModelScope)
- 80%+ ML model user base coverage
- Manifest-First Architecture

### âœ… Phase 2: Universal Inference (Complete)
- **Axon v3.0.0**: Universal ONNX conversion
- **Core v2.2.0-alpha**: Multi-type tensor support, named inputs
- **Axon v3.1.0**: Vision model support
- **Axon v3.1.3**: Enhanced seq2seq/multi-encoder model support, improved T5 conversion
- **Core v3.2.8-alpha**: Complete seq2seq inference with auto attention_mask generation
- E2E validation with automated testing

### ğŸš§ Phase 3: Production Readiness (In Progress)
- **Timeline**: Q1-Q2 2026
- gRPC full implementation
- GPU memory management
- Batched inference optimization
- Model caching improvements

### ğŸ“… Phase 4: Ecosystem Expansion (Planned)
- **Timeline**: Q2-Q3 2026
- MLOS Linux Distributions (Ubuntu & Flatcar-based)
- Additional repository adapters (Replicate, Kaggle)
- Cloud-native deployment options
- Model marketplace integration

## ğŸ”’ Patent Status

| Patent | Status | Key Innovation |
|--------|--------|----------------|
| **US-63/861,527** | Pending | Model Package Format (MPF), Native lifecycle management |
| **US-63/865,176** | Pending (CIP) | ML-aware scheduler, Tensor memory management, GPU orchestration |

## ğŸ“š Repositories

### Core Projects
- **[axon](https://github.com/mlOS-foundation/axon)** - Universal model installer & ONNX converter
- **[core-releases](https://github.com/mlOS-foundation/core-releases)** - Public releases for MLOS Core
- **[system-test](https://github.com/mlOS-foundation/system-test)** - E2E validation framework
- **[mlosfoundation.org](https://github.com/mlOS-foundation/mlosfoundation.org)** - Official website

### Distribution Projects (Planning)
- **[mlos-linux-ubuntu](https://github.com/mlOS-foundation/mlos-linux-ubuntu)** - Ubuntu-based MLOS distribution
- **[mlos-linux-flatcar](https://github.com/mlOS-foundation/mlos-linux-flatcar)** - Flatcar-based MLOS distribution
- **[mlos-linux-kernel](https://github.com/mlOS-foundation/mlos-linux-kernel)** - Shared kernel patches

### Supporting Projects
- **[smi-spec](https://github.com/mlOS-foundation/smi-spec)** - Standard Model Interface specification
- **[bindings](https://github.com/mlOS-foundation/bindings)** - Language bindings for SMI protocol
- **[examples](https://github.com/mlOS-foundation/examples)** - Examples, tutorials, and best practices

## ğŸŒŸ Why MLOS?

### The Problem
- **Framework Lock-in**: Models tied to specific frameworks
- **Deployment Complexity**: Different platforms require different integrations
- **No Standard Interface**: Each framework has its own API
- **Resource Inefficiency**: No unified resource management

### The Solution
- **Universal Interface**: SMI provides framework-agnostic contract
- **Standardized Packaging**: MPF (Model Package Format) via Axon
- **Universal Execution**: ONNX-based inference for all models
- **Kernel-Level Optimization**: OS-level ML resource management

## ğŸ“ Contact & Resources

- **Website**: [mlosfoundation.org](https://mlosfoundation.org)
- **E2E Reports**: [mlos-foundation.github.io/system-test](https://mlos-foundation.github.io/system-test/)
- **Email**: info@mlos-foundation.org
- **Status**: Alpha Development
- **Public Release**: Q2 2026

## ğŸ“„ License

MLOS Foundation projects use the following licenses:
- **Axon**: GNU AGPL v3.0
- **MLOS Core**: MIT License
- **Documentation**: MIT License

---

**Â© 2025 mlOS Foundation. Building the future of ML infrastructure.**

*"Just as POSIX made software portable across Unix systems, MLOS makes ML models portable across any framework and deployment environment."*

**Signal. Propagate. Myelinate.** ğŸ§ 
